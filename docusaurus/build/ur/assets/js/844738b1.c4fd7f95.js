"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[5618],{5059(n,e,r){r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4/voice-to-action","title":"Voice-to-Action","description":"Voice-to-action systems \u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u0648 spoken commands \u0633\u0645\u062c\u06be\u0646\u06d2 \u0627\u0648\u0631 \u0627\u0646\u06c1\u06cc\u06ba robotic actions \u0645\u06cc\u06ba \u062a\u0628\u062f\u06cc\u0644 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0642\u0627\u0628\u0644 \u0628\u0646\u0627\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4 \u06cc\u06c1 \u0679\u06cc\u06a9\u0646\u0627\u0644\u0648\u062c\u06cc human-robot interaction \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u06cc\u06a9 natural interface \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u06cc \u06c1\u06d2\u06d4","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/module-4/voice-to-action.md","sourceDirName":"module-4","slug":"/module-4/voice-to-action","permalink":"/physical-ai-humanoid-robotics/ur/docs/module-4/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-humanoid-robotics/edit/main/docusaurus/docs/module-4/voice-to-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Voice-to-Action"},"sidebar":"tutorialSidebar","previous":{"title":"\u0645\u0627\u0688\u06cc\u0648\u0644 4 - Vision-Language-Action (VLA)","permalink":"/physical-ai-humanoid-robotics/ur/docs/module-4/"},"next":{"title":"LLM Cognitive Planning","permalink":"/physical-ai-humanoid-robotics/ur/docs/module-4/llm-planning"}}');var o=r(4848),t=r(8453);const a={sidebar_position:2,title:"Voice-to-Action"},s="\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 Voice-to-Action Systems",c={},l=[{value:"\u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0645\u0642\u0627\u0635\u062f",id:"\u0633\u06cc\u06a9\u06be\u0646\u06d2-\u06a9\u06d2-\u0645\u0642\u0627\u0635\u062f",level:2},{value:"Voice-to-Action Systems \u06a9\u0627 \u062a\u0639\u0627\u0631\u0641",id:"voice-to-action-systems-\u06a9\u0627-\u062a\u0639\u0627\u0631\u0641",level:2},{value:"OpenAI Whisper \u06a9\u06d2 \u0633\u0627\u062a\u06be Speech Recognition",id:"openai-whisper-\u06a9\u06d2-\u0633\u0627\u062a\u06be-speech-recognition",level:2},{value:"Basic Whisper Integration",id:"basic-whisper-integration",level:3},{value:"Command Processing \u0627\u0648\u0631 Action Planning",id:"command-processing-\u0627\u0648\u0631-action-planning",level:2},{value:"Natural Language Command Parser",id:"natural-language-command-parser",level:3},{value:"Error Handling \u0627\u0648\u0631 Fallback Mechanisms",id:"error-handling-\u0627\u0648\u0631-fallback-mechanisms",level:2},{value:"Robust Command Processing",id:"robust-command-processing",level:3},{value:"\u0645\u0634\u0642",id:"\u0645\u0634\u0642",level:2},{value:"\u062e\u0644\u0627\u0635\u06c1",id:"\u062e\u0644\u0627\u0635\u06c1",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688-\u0631\u0648\u0628\u0648\u0679\u0633-\u06a9\u06d2-\u0644\u06cc\u06d2-voice-to-action-systems",children:"\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 Voice-to-Action Systems"})}),"\n",(0,o.jsx)(e.p,{children:"Voice-to-action systems \u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u0648 spoken commands \u0633\u0645\u062c\u06be\u0646\u06d2 \u0627\u0648\u0631 \u0627\u0646\u06c1\u06cc\u06ba robotic actions \u0645\u06cc\u06ba \u062a\u0628\u062f\u06cc\u0644 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0642\u0627\u0628\u0644 \u0628\u0646\u0627\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4 \u06cc\u06c1 \u0679\u06cc\u06a9\u0646\u0627\u0644\u0648\u062c\u06cc human-robot interaction \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u06cc\u06a9 natural interface \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u06cc \u06c1\u06d2\u06d4"}),"\n",(0,o.jsx)(e.h2,{id:"\u0633\u06cc\u06a9\u06be\u0646\u06d2-\u06a9\u06d2-\u0645\u0642\u0627\u0635\u062f",children:"\u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0645\u0642\u0627\u0635\u062f"}),"\n",(0,o.jsx)(e.p,{children:"\u0627\u0633 \u0633\u0628\u0642 \u06a9\u0648 \u0645\u06a9\u0645\u0644 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0628\u0639\u062f\u060c \u0622\u067e \u0642\u0627\u0628\u0644 \u06c1\u0648\u06ba \u06af\u06d2:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 speech recognition systems \u0644\u0627\u06af\u0648 \u06a9\u0631\u0646\u0627"}),"\n",(0,o.jsx)(e.li,{children:"Robotic execution \u06a9\u06d2 \u0644\u06cc\u06d2 natural language commands process \u06a9\u0631\u0646\u0627"}),"\n",(0,o.jsx)(e.li,{children:"Speech recognition errors \u0627\u0648\u0631 fallback mechanisms handle \u06a9\u0631\u0646\u0627"}),"\n",(0,o.jsx)(e.li,{children:"Voice commands \u06a9\u0648 robotic action planning \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0627\u0646\u0636\u0645\u0627\u0645 \u06a9\u0631\u0646\u0627"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"voice-to-action-systems-\u06a9\u0627-\u062a\u0639\u0627\u0631\u0641",children:"Voice-to-Action Systems \u06a9\u0627 \u062a\u0639\u0627\u0631\u0641"}),"\n",(0,o.jsx)(e.p,{children:"\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 voice-to-action systems \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Speech Recognition"}),": Spoken language \u06a9\u0648 text \u0645\u06cc\u06ba \u062a\u0628\u062f\u06cc\u0644 \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Natural Language Understanding"}),": Commands \u06a9\u06d2 \u0645\u0639\u0646\u06cc \u06a9\u06cc \u062a\u0634\u0631\u06cc\u062d"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Planning"}),": Commands \u06a9\u0648 robotic actions \u0645\u06cc\u06ba \u062a\u0628\u062f\u06cc\u0644 \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Execution Monitoring"}),": \u06cc\u0642\u06cc\u0646\u06cc \u0628\u0646\u0627\u0646\u0627 \u06a9\u06c1 actions \u0635\u062d\u06cc\u062d \u0637\u0631\u06cc\u0642\u06d2 \u0633\u06d2 \u0627\u0646\u062c\u0627\u0645 \u062f\u06cc\u06d2 \u062c\u0627\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2\u060c voice-to-action systems \u06a9\u0648 handle \u06a9\u0631\u0646\u0627 \u0636\u0631\u0648\u0631\u06cc \u06c1\u06d2:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Noisy environments"}),"\n",(0,o.jsx)(e.li,{children:"Multiple speakers"}),"\n",(0,o.jsx)(e.li,{children:"\u067e\u06cc\u0686\u06cc\u062f\u06c1 command structures"}),"\n",(0,o.jsx)(e.li,{children:"\u0631\u06cc\u0626\u0644-\u0679\u0627\u0626\u0645 processing requirements"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"openai-whisper-\u06a9\u06d2-\u0633\u0627\u062a\u06be-speech-recognition",children:"OpenAI Whisper \u06a9\u06d2 \u0633\u0627\u062a\u06be Speech Recognition"}),"\n",(0,o.jsx)(e.h3,{id:"basic-whisper-integration",children:"Basic Whisper Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import AudioData\r\nimport openai\r\nimport numpy as np\r\nimport pyaudio\r\nimport wave\r\nimport threading\r\nimport queue\r\n\r\nclass VoiceToActionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_to_action_node')\r\n\r\n        # Publishers and subscribers\r\n        self.command_pub = self.create_publisher(String, '/robot_commands', 10)\r\n        self.status_pub = self.create_publisher(String, '/voice_status', 10)\r\n\r\n        # Audio recording parameters\r\n        self.chunk = 1024\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.rate = 44100\r\n        self.record_seconds = 5\r\n\r\n        # Initialize audio stream\r\n        self.audio = pyaudio.PyAudio()\r\n        self.audio_queue = queue.Queue()\r\n\r\n        # Start audio recording thread\r\n        self.recording = True\r\n        self.recording_thread = threading.Thread(target=self.record_audio)\r\n        self.recording_thread.start()\r\n\r\n        # Timer for processing audio\r\n        self.timer = self.create_timer(1.0, self.process_audio)\r\n\r\n    def record_audio(self):\r\n        \"\"\"Record audio from microphone\"\"\"\r\n        stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n\r\n        while self.recording:\r\n            data = stream.read(self.chunk)\r\n            self.audio_queue.put(data)\r\n\r\n        stream.stop_stream()\r\n        stream.close()\r\n\r\n    def process_audio(self):\r\n        \"\"\"Process recorded audio and convert to text\"\"\"\r\n        if not self.audio_queue.empty():\r\n            # Collect audio frames\r\n            frames = []\r\n            while not self.audio_queue.empty():\r\n                frames.append(self.audio_queue.get())\r\n            \r\n            # Convert to audio file format\r\n            audio_data = b''.join(frames)\r\n            \r\n            # Use Whisper API for transcription\r\n            # In practice, you might use local Whisper model\r\n            try:\r\n                # Save to temporary file\r\n                temp_file = \"temp_audio.wav\"\r\n                with wave.open(temp_file, 'wb') as wf:\r\n                    wf.setnchannels(self.channels)\r\n                    wf.setsampwidth(self.audio.get_sample_size(self.format))\r\n                    wf.setframerate(self.rate)\r\n                    wf.writeframes(audio_data)\r\n                \r\n                # Transcribe using Whisper\r\n                with open(temp_file, 'rb') as audio_file:\r\n                    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\r\n                    text = transcript[\"text\"]\r\n                    \r\n                    # Publish command\r\n                    command_msg = String()\r\n                    command_msg.data = text\r\n                    self.command_pub.publish(command_msg)\r\n                    \r\n                    self.get_logger().info(f'Recognized: {text}')\r\n            except Exception as e:\r\n                self.get_logger().error(f'Speech recognition error: {e}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    voice_node = VoiceToActionNode()\r\n\r\n    try:\r\n        rclpy.spin(voice_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        voice_node.recording = False\r\n        voice_node.recording_thread.join()\r\n        voice_node.audio.terminate()\r\n        voice_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"command-processing-\u0627\u0648\u0631-action-planning",children:"Command Processing \u0627\u0648\u0631 Action Planning"}),"\n",(0,o.jsx)(e.h3,{id:"natural-language-command-parser",children:"Natural Language Command Parser"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class CommandParser:\r\n    def __init__(self):\r\n        self.action_keywords = {\r\n            'move': 'navigation',\r\n            'go': 'navigation',\r\n            'walk': 'navigation',\r\n            'pick': 'manipulation',\r\n            'grasp': 'manipulation',\r\n            'bring': 'manipulation',\r\n            'speak': 'communication',\r\n            'say': 'communication'\r\n        }\r\n    \r\n    def parse_command(self, text):\r\n        \"\"\"Parse natural language command into structured action\"\"\"\r\n        text_lower = text.lower()\r\n        \r\n        # Identify action type\r\n        action_type = None\r\n        for keyword, action in self.action_keywords.items():\r\n            if keyword in text_lower:\r\n                action_type = action\r\n                break\r\n        \r\n        # Extract target or location\r\n        # This is simplified - real implementation would use NLP\r\n        target = self.extract_target(text_lower)\r\n        \r\n        return {\r\n            'action_type': action_type,\r\n            'target': target,\r\n            'original_text': text\r\n        }\r\n    \r\n    def extract_target(self, text):\r\n        \"\"\"Extract target object or location from command\"\"\"\r\n        # Simplified extraction\r\n        # Real implementation would use named entity recognition\r\n        common_targets = ['cup', 'book', 'phone', 'kitchen', 'living room']\r\n        for target in common_targets:\r\n            if target in text:\r\n                return target\r\n        return None\n"})}),"\n",(0,o.jsx)(e.h2,{id:"error-handling-\u0627\u0648\u0631-fallback-mechanisms",children:"Error Handling \u0627\u0648\u0631 Fallback Mechanisms"}),"\n",(0,o.jsx)(e.h3,{id:"robust-command-processing",children:"Robust Command Processing"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class RobustCommandProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'robust_command_processor\')\r\n        \r\n        self.command_sub = self.create_subscription(\r\n            String,\r\n            \'/robot_commands\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n        \r\n        self.confirmation_pub = self.create_publisher(\r\n            String,\r\n            \'/command_confirmation\',\r\n            10\r\n        )\r\n        \r\n        self.parser = CommandParser()\r\n    \r\n    def command_callback(self, msg):\r\n        """Process command with error handling"""\r\n        try:\r\n            parsed = self.parser.parse_command(msg.data)\r\n            \r\n            if parsed[\'action_type\'] is None:\r\n                # Ask for clarification\r\n                self.request_clarification(msg.data)\r\n            else:\r\n                # Confirm and execute\r\n                self.confirm_and_execute(parsed)\r\n                \r\n        except Exception as e:\r\n            self.get_logger().error(f\'Command processing error: {e}\')\r\n            self.handle_error(msg.data, str(e))\r\n    \r\n    def request_clarification(self, command):\r\n        """Request clarification for ambiguous commands"""\r\n        confirmation = String()\r\n        confirmation.data = f"Could you clarify: {command}?"\r\n        self.confirmation_pub.publish(confirmation)\r\n    \r\n    def confirm_and_execute(self, parsed_command):\r\n        """Confirm command and initiate execution"""\r\n        confirmation = String()\r\n        confirmation.data = f"Executing: {parsed_command[\'action_type\']} for {parsed_command[\'target\']}"\r\n        self.confirmation_pub.publish(confirmation)\r\n        # Execute command...\n'})}),"\n",(0,o.jsx)(e.h2,{id:"\u0645\u0634\u0642",children:"\u0645\u0634\u0642"}),"\n",(0,o.jsx)(e.p,{children:"\u0627\u06cc\u06a9 voice-to-action system \u0644\u0627\u06af\u0648 \u06a9\u0631\u06cc\u06ba \u062c\u0648:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Microphone \u0633\u06d2 audio record \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n",(0,o.jsx)(e.li,{children:"Whisper \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 speech \u06a9\u0648 text \u0645\u06cc\u06ba \u062a\u0628\u062f\u06cc\u0644 \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n",(0,o.jsx)(e.li,{children:"Natural language commands parse \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n",(0,o.jsx)(e.li,{children:"Appropriate robotic actions generate \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n",(0,o.jsx)(e.li,{children:"Errors handle \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u0627\u0648\u0631 clarification request \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"\u062e\u0644\u0627\u0635\u06c1",children:"\u062e\u0644\u0627\u0635\u06c1"}),"\n",(0,o.jsx)(e.p,{children:"Voice-to-action systems \u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 natural human-robot interaction \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4 Speech recognition\u060c natural language understanding\u060c \u0627\u0648\u0631 action planning \u06a9\u0627 \u0645\u0646\u0627\u0633\u0628 \u0627\u0646\u0636\u0645\u0627\u0645 intelligent \u0631\u0648\u0628\u0648\u0679\u0633 \u0628\u0646\u0627\u062a\u0627 \u06c1\u06d2 \u062c\u0648 complex voice commands \u06a9\u06cc \u067e\u06cc\u0631\u0648\u06cc \u06a9\u0631 \u0633\u06a9\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4 \u0627\u06af\u0644\u06d2 \u0633\u0628\u0642 \u0645\u06cc\u06ba\u060c \u06c1\u0645 cognitive planning \u06a9\u06d2 \u0644\u06cc\u06d2 LLM integration \u06a9\u0648 \u062f\u0631\u06cc\u0627\u0641\u062a \u06a9\u0631\u06cc\u06ba \u06af\u06d2\u06d4"})]})}function m(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453(n,e,r){r.d(e,{R:()=>a,x:()=>s});var i=r(6540);const o={},t=i.createContext(o);function a(n){const e=i.useContext(t);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),i.createElement(t.Provider,{value:e},n.children)}}}]);