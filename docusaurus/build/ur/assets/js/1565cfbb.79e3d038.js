"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[3121],{6694(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-4/llm-planning","title":"LLM Cognitive Planning","description":"Large Language Models (LLMs) cognitive planning capabilities \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba \u062c\u0648 \u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u0648 \u067e\u06cc\u0686\u06cc\u062f\u06c1 natural language commands \u0633\u0645\u062c\u06be\u0646\u06d2 \u0627\u0648\u0631 appropriate action sequences generate \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0642\u0627\u0628\u0644 \u0628\u0646\u0627\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4 \u06cc\u06c1 \u0633\u0628\u0642 robotic task planning \u06a9\u06d2 \u0644\u06cc\u06d2 LLMs \u06a9\u06d2 \u0627\u0646\u0636\u0645\u0627\u0645 \u06a9\u0627 \u0627\u062d\u0627\u0637\u06c1 \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/module-4/llm-planning.md","sourceDirName":"module-4","slug":"/module-4/llm-planning","permalink":"/physical-ai-humanoid-robotics/ur/docs/module-4/llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-humanoid-robotics/edit/main/docusaurus/docs/module-4/llm-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"LLM Cognitive Planning"},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action","permalink":"/physical-ai-humanoid-robotics/ur/docs/module-4/voice-to-action"},"next":{"title":"Capstone Project","permalink":"/physical-ai-humanoid-robotics/ur/docs/module-4/capstone-project"}}');var t=i(4848),s=i(8453);const a={sidebar_position:3,title:"LLM Cognitive Planning"},o="\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 LLM Cognitive Planning",l={},c=[{value:"\u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0645\u0642\u0627\u0635\u062f",id:"\u0633\u06cc\u06a9\u06be\u0646\u06d2-\u06a9\u06d2-\u0645\u0642\u0627\u0635\u062f",level:2},{value:"LLM Cognitive Planning \u06a9\u0627 \u062a\u0639\u0627\u0631\u0641",id:"llm-cognitive-planning-\u06a9\u0627-\u062a\u0639\u0627\u0631\u0641",level:2},{value:"LLM Integration Architecture",id:"llm-integration-architecture",level:2},{value:"Basic LLM Integration",id:"basic-llm-integration",level:3},{value:"Multimodal Reasoning with Vision",id:"multimodal-reasoning-with-vision",level:2},{value:"Vision-Language Integration",id:"vision-language-integration",level:3},{value:"\u0645\u0634\u0642",id:"\u0645\u0634\u0642",level:2},{value:"\u062e\u0644\u0627\u0635\u06c1",id:"\u062e\u0644\u0627\u0635\u06c1",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688-\u0631\u0648\u0628\u0648\u0679\u0633-\u06a9\u06d2-\u0644\u06cc\u06d2-llm-cognitive-planning",children:"\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 LLM Cognitive Planning"})}),"\n",(0,t.jsx)(e.p,{children:"Large Language Models (LLMs) cognitive planning capabilities \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba \u062c\u0648 \u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u0648 \u067e\u06cc\u0686\u06cc\u062f\u06c1 natural language commands \u0633\u0645\u062c\u06be\u0646\u06d2 \u0627\u0648\u0631 appropriate action sequences generate \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0642\u0627\u0628\u0644 \u0628\u0646\u0627\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4 \u06cc\u06c1 \u0633\u0628\u0642 robotic task planning \u06a9\u06d2 \u0644\u06cc\u06d2 LLMs \u06a9\u06d2 \u0627\u0646\u0636\u0645\u0627\u0645 \u06a9\u0627 \u0627\u062d\u0627\u0637\u06c1 \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4"}),"\n",(0,t.jsx)(e.h2,{id:"\u0633\u06cc\u06a9\u06be\u0646\u06d2-\u06a9\u06d2-\u0645\u0642\u0627\u0635\u062f",children:"\u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0645\u0642\u0627\u0635\u062f"}),"\n",(0,t.jsx)(e.p,{children:"\u0627\u0633 \u0633\u0628\u0642 \u06a9\u0648 \u0645\u06a9\u0645\u0644 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0628\u0639\u062f\u060c \u0622\u067e \u0642\u0627\u0628\u0644 \u06c1\u0648\u06ba \u06af\u06d2:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Cognitive planning \u06a9\u06d2 \u0644\u06cc\u06d2 LLMs \u06a9\u0648 \u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679 systems \u06a9\u06d2 \u0633\u0627\u062a\u06be \u0627\u0646\u0636\u0645\u0627\u0645 \u06a9\u0631\u0646\u0627"}),"\n",(0,t.jsx)(e.li,{children:"\u0645\u0624\u062b\u0631 robotic task planning \u06a9\u06d2 \u0644\u06cc\u06d2 prompts \u0688\u06cc\u0632\u0627\u0626\u0646 \u06a9\u0631\u0646\u0627"}),"\n",(0,t.jsx)(e.li,{children:"Vision \u0627\u0648\u0631 language \u06a9\u0648 \u0645\u0644\u0627 \u06a9\u0631 multimodal reasoning \u0644\u0627\u06af\u0648 \u06a9\u0631\u0646\u0627"}),"\n",(0,t.jsx)(e.li,{children:"LLM planning \u06a9\u06d2 \u0633\u0627\u062a\u06be \u067e\u06cc\u0686\u06cc\u062f\u06c1\u060c multi-step commands handle \u06a9\u0631\u0646\u0627"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"llm-cognitive-planning-\u06a9\u0627-\u062a\u0639\u0627\u0631\u0641",children:"LLM Cognitive Planning \u06a9\u0627 \u062a\u0639\u0627\u0631\u0641"}),"\n",(0,t.jsx)(e.p,{children:"\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2 LLM cognitive planning \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Natural Language Understanding"}),": \u067e\u06cc\u0686\u06cc\u062f\u06c1 commands \u06a9\u06cc \u062a\u0634\u0631\u06cc\u062d"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Decomposition"}),": \u067e\u06cc\u0686\u06cc\u062f\u06c1 \u06a9\u0627\u0645\u0648\u06ba \u06a9\u0648 executable steps \u0645\u06cc\u06ba \u062a\u0648\u0691\u0646\u0627"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"World Modeling"}),": Environment \u0627\u0648\u0631 \u0631\u0648\u0628\u0648\u0679 capabilities \u06a9\u0648 \u0633\u0645\u062c\u06be\u0646\u0627"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action Sequencing"}),": Appropriate action plans generate \u06a9\u0631\u0646\u0627"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Execution Monitoring"}),": Feedback \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u067e\u0631 plans adapt \u06a9\u0631\u0646\u0627"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"\u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u06d2 \u0644\u06cc\u06d2\u060c LLM planning \u06a9\u0648 \u063a\u0648\u0631 \u06a9\u0631\u0646\u0627 \u0686\u0627\u06c1\u06cc\u06d2:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"\u062c\u0633\u0645\u0627\u0646\u06cc constraints \u0627\u0648\u0631 capabilities"}),"\n",(0,t.jsx)(e.li,{children:"Environmental context"}),"\n",(0,t.jsx)(e.li,{children:"Safety requirements"}),"\n",(0,t.jsx)(e.li,{children:"\u0631\u06cc\u0626\u0644-\u0679\u0627\u0626\u0645 execution constraints"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"llm-integration-architecture",children:"LLM Integration Architecture"}),"\n",(0,t.jsx)(e.h3,{id:"basic-llm-integration",children:"Basic LLM Integration"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Pose\r\nimport openai\r\nimport json\r\nimport time\r\n\r\nclass LLMPlanningNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'llm_planning_node\')\r\n\r\n        # Publishers and subscribers\r\n        self.command_pub = self.create_publisher(String, \'/robot_commands\', 10)\r\n        self.plan_pub = self.create_publisher(String, \'/planning_output\', 10)\r\n\r\n        # Subscription for high-level commands\r\n        self.command_sub = self.create_subscription(\r\n            String,\r\n            \'/high_level_commands\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n\r\n        # Initialize OpenAI client\r\n        self.client = openai.OpenAI()\r\n\r\n        # Robot capabilities and environment context\r\n        self.robot_context = {\r\n            "capabilities": [\r\n                "navigation",\r\n                "object manipulation",\r\n                "speech",\r\n                "gesture",\r\n                "grasping"\r\n            ],\r\n            "environment": {\r\n                "locations": ["kitchen", "living room", "bedroom", "office"],\r\n                "objects": ["cup", "book", "phone", "keys", "bottle"],\r\n                "constraints": {\r\n                    "max_load": "2kg",\r\n                    "max_height": "1.8m",\r\n                    "safety_radius": "0.5m"\r\n                }\r\n            }\r\n        }\r\n\r\n    def command_callback(self, msg):\r\n        """Process high-level command using LLM"""\r\n        command = msg.data\r\n        self.get_logger().info(f\'Received command: {command}\')\r\n\r\n        try:\r\n            # Generate plan using LLM\r\n            plan = self.generate_plan_with_llm(command)\r\n            self.publish_plan(plan)\r\n        except Exception as e:\r\n            self.get_logger().error(f\'LLM planning error: {e}\')\r\n            self.handle_planning_error(command, str(e))\r\n\r\n    def generate_plan_with_llm(self, command):\r\n        """Generate execution plan using LLM"""\r\n        system_prompt = f"""\r\n        You are a planning assistant for a humanoid robot. The robot has the following capabilities: {self.robot_context[\'capabilities\']}.\r\n        The environment contains these locations: {self.robot_context[\'environment\'][\'locations\']} and objects: {self.robot_context[\'environment\'][\'objects\']}.\r\n        The robot has these constraints: {self.robot_context[\'environment\'][\'constraints\']}.\r\n\r\n        Your task is to decompose natural language commands into executable steps for the robot.\r\n        Return a JSON object with the following structure:\r\n        {{\r\n            "command": "original command",\r\n            "steps": [\r\n                {{\r\n                    "action": "action_type",\r\n                    "parameters": {{"param1": "value1", ...}},\r\n                    "description": "brief description"\r\n                }}\r\n            ],\r\n            "estimated_duration": "estimated time in seconds"\r\n        }}\r\n\r\n        Action types: \'navigate\', \'grasp\', \'place\', \'speak\', \'gesture\', \'wait\', \'detect_object\'\r\n        """\r\n\r\n        user_prompt = f"Command: {command}"\r\n\r\n        response = self.client.chat.completions.create(\r\n            model="gpt-3.5-turbo",\r\n            messages=[\r\n                {"role": "system", "content": system_prompt},\r\n                {"role": "user", "content": user_prompt}\r\n            ],\r\n            temperature=0.1,\r\n            max_tokens=500\r\n        )\r\n\r\n        # Parse the response\r\n        response_text = response.choices[0].message.content\r\n        # Extract and parse JSON...\r\n        return plan\r\n\r\n    def publish_plan(self, plan):\r\n        """Publish the generated plan"""\r\n        plan_msg = String()\r\n        plan_msg.data = json.dumps(plan)\r\n        self.plan_pub.publish(plan_msg)\r\n        self.execute_plan(plan)\r\n\r\n    def execute_plan(self, plan):\r\n        """Execute the plan steps"""\r\n        for step in plan[\'steps\']:\r\n            self.get_logger().info(f\'Executing: {step["description"]}\')\r\n            cmd_msg = String()\r\n            cmd_msg.data = json.dumps(step)\r\n            self.command_pub.publish(cmd_msg)\r\n            time.sleep(2)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"multimodal-reasoning-with-vision",children:"Multimodal Reasoning with Vision"}),"\n",(0,t.jsx)(e.h3,{id:"vision-language-integration",children:"Vision-Language Integration"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import cv2\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\n\r\nclass MultimodalLLMPlanner:\r\n    def __init__(self, node):\r\n        self.node = node\r\n        self.bridge = CvBridge()\r\n\r\n        # Subscribe to camera feed\r\n        self.image_sub = node.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.latest_image = None\r\n\r\n    def generate_vision_guided_plan(self, command, image=None):\r\n        """Generate plan with visual context"""\r\n        if image is None:\r\n            image = self.latest_image\r\n\r\n        if image is not None:\r\n            image_description = self.describe_image_content(image)\r\n            # Use visual information in planning...\n'})}),"\n",(0,t.jsx)(e.h2,{id:"\u0645\u0634\u0642",children:"\u0645\u0634\u0642"}),"\n",(0,t.jsx)(e.p,{children:"\u0627\u06cc\u06a9 LLM-based cognitive planning system \u0644\u0627\u06af\u0648 \u06a9\u0631\u06cc\u06ba \u062c\u0648:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Natural language commands \u06a9\u0648 parse \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n",(0,t.jsx)(e.li,{children:"Complex tasks \u06a9\u0648 executable steps \u0645\u06cc\u06ba decompose \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n",(0,t.jsx)(e.li,{children:"Visual context \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n",(0,t.jsx)(e.li,{children:"Execution plans generate \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n",(0,t.jsx)(e.li,{children:"Errors handle \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u0627\u0648\u0631 fallback plans \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"\u062e\u0644\u0627\u0635\u06c1",children:"\u062e\u0644\u0627\u0635\u06c1"}),"\n",(0,t.jsx)(e.p,{children:"LLM cognitive planning \u06c1\u06cc\u0648\u0645\u06cc\u0646\u0648\u0626\u0688 \u0631\u0648\u0628\u0648\u0679\u0633 \u06a9\u0648 \u067e\u06cc\u0686\u06cc\u062f\u06c1 natural language commands \u0633\u0645\u062c\u06be\u0646\u06d2 \u0627\u0648\u0631 appropriate action sequences generate \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0642\u0627\u0628\u0644 \u0628\u0646\u0627\u062a\u0627 \u06c1\u06d2\u06d4 Proper prompt design \u0627\u0648\u0631 multimodal reasoning intelligent planning systems \u0628\u0646\u0627\u062a\u06d2 \u06c1\u06cc\u06ba \u062c\u0648 complex tasks handle \u06a9\u0631 \u0633\u06a9\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4 \u0627\u06af\u0644\u06d2 \u0633\u0628\u0642 \u0645\u06cc\u06ba\u060c \u06c1\u0645 \u062a\u0645\u0627\u0645 modules \u06a9\u0648 integrate \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 capstone project \u06a9\u0648 \u062f\u0631\u06cc\u0627\u0641\u062a \u06a9\u0631\u06cc\u06ba \u06af\u06d2\u06d4"})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>a,x:()=>o});var r=i(6540);const t={},s=r.createContext(t);function a(n){const e=r.useContext(s);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),r.createElement(s.Provider,{value:e},n.children)}}}]);