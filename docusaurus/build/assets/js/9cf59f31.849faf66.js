"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[9929],{6135(n,e,t){t.r(e),t.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4/llm-planning","title":"LLM Cognitive Planning","description":"Large Language Models (LLMs) provide cognitive planning capabilities that enable humanoid robots to understand complex natural language commands and generate appropriate action sequences. This lesson covers the integration of LLMs for robotic task planning.","source":"@site/docs/module-4/llm-planning.md","sourceDirName":"module-4","slug":"/module-4/llm-planning","permalink":"/physical-ai-humanoid-robotics/docs/module-4/llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-humanoid-robotics/edit/main/docusaurus/docs/module-4/llm-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"LLM Cognitive Planning"},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action","permalink":"/physical-ai-humanoid-robotics/docs/module-4/voice-to-action"},"next":{"title":"Capstone Project","permalink":"/physical-ai-humanoid-robotics/docs/module-4/capstone-project"}}');var i=t(4848),s=t(8453);const o={sidebar_position:3,title:"LLM Cognitive Planning"},l="LLM Cognitive Planning for Humanoid Robots",r={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to LLM Cognitive Planning",id:"introduction-to-llm-cognitive-planning",level:2},{value:"LLM Integration Architecture",id:"llm-integration-architecture",level:2},{value:"Basic LLM Integration",id:"basic-llm-integration",level:3},{value:"Multimodal Reasoning with Vision",id:"multimodal-reasoning-with-vision",level:2},{value:"Vision-Language Integration",id:"vision-language-integration",level:3},{value:"Context-Aware Planning",id:"context-aware-planning",level:2},{value:"Maintaining Planning Context",id:"maintaining-planning-context",level:3},{value:"Advanced Planning Strategies",id:"advanced-planning-strategies",level:2},{value:"Hierarchical Task Planning",id:"hierarchical-task-planning",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:2},{value:"Plan Validation Framework",id:"plan-validation-framework",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching and Pre-planning",id:"caching-and-pre-planning",level:3},{value:"Exercise",id:"exercise",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"llm-cognitive-planning-for-humanoid-robots",children:"LLM Cognitive Planning for Humanoid Robots"})}),"\n",(0,i.jsx)(e.p,{children:"Large Language Models (LLMs) provide cognitive planning capabilities that enable humanoid robots to understand complex natural language commands and generate appropriate action sequences. This lesson covers the integration of LLMs for robotic task planning."}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(e.p,{children:"After completing this lesson, you will be able to:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Integrate LLMs with humanoid robot systems for cognitive planning"}),"\n",(0,i.jsx)(e.li,{children:"Design prompts for effective robotic task planning"}),"\n",(0,i.jsx)(e.li,{children:"Implement multimodal reasoning combining vision and language"}),"\n",(0,i.jsx)(e.li,{children:"Handle complex, multi-step commands with LLM planning"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-llm-cognitive-planning",children:"Introduction to LLM Cognitive Planning"}),"\n",(0,i.jsx)(e.p,{children:"LLM cognitive planning for humanoid robots involves:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Natural Language Understanding"}),": Interpreting complex commands"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task Decomposition"}),": Breaking complex tasks into executable steps"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"World Modeling"}),": Understanding the environment and robot capabilities"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action Sequencing"}),": Generating appropriate action plans"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Execution Monitoring"}),": Adapting plans based on feedback"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"For humanoid robots, LLM planning must consider:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Physical constraints and capabilities"}),"\n",(0,i.jsx)(e.li,{children:"Environmental context"}),"\n",(0,i.jsx)(e.li,{children:"Safety requirements"}),"\n",(0,i.jsx)(e.li,{children:"Real-time execution constraints"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"llm-integration-architecture",children:"LLM Integration Architecture"}),"\n",(0,i.jsx)(e.h3,{id:"basic-llm-integration",children:"Basic LLM Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nimport openai\nimport json\nimport time\n\nclass LLMPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_planning_node\')\n\n        # Publishers and subscribers\n        self.command_pub = self.create_publisher(String, \'/robot_commands\', 10)\n        self.plan_pub = self.create_publisher(String, \'/planning_output\', 10)\n\n        # Subscription for high-level commands\n        self.command_sub = self.create_subscription(\n            String,\n            \'/high_level_commands\',\n            self.command_callback,\n            10\n        )\n\n        # Initialize OpenAI client\n        # In practice, you might use local models or different APIs\n        self.client = openai.OpenAI()\n\n        # Robot capabilities and environment context\n        self.robot_context = {\n            "capabilities": [\n                "navigation",\n                "object manipulation",\n                "speech",\n                "gesture",\n                "grasping"\n            ],\n            "environment": {\n                "locations": ["kitchen", "living room", "bedroom", "office"],\n                "objects": ["cup", "book", "phone", "keys", "bottle"],\n                "constraints": {\n                    "max_load": "2kg",\n                    "max_height": "1.8m",\n                    "safety_radius": "0.5m"\n                }\n            }\n        }\n\n    def command_callback(self, msg):\n        """Process high-level command using LLM"""\n        command = msg.data\n        self.get_logger().info(f\'Received command: {command}\')\n\n        try:\n            # Generate plan using LLM\n            plan = self.generate_plan_with_llm(command)\n            self.publish_plan(plan)\n        except Exception as e:\n            self.get_logger().error(f\'LLM planning error: {e}\')\n            self.handle_planning_error(command, str(e))\n\n    def generate_plan_with_llm(self, command):\n        """Generate execution plan using LLM"""\n        system_prompt = f"""\n        You are a planning assistant for a humanoid robot. The robot has the following capabilities: {self.robot_context[\'capabilities\']}.\n        The environment contains these locations: {self.robot_context[\'environment\'][\'locations\']} and objects: {self.robot_context[\'environment\'][\'objects\']}.\n        The robot has these constraints: {self.robot_context[\'environment\'][\'constraints\']}.\n\n        Your task is to decompose natural language commands into executable steps for the robot.\n        Return a JSON object with the following structure:\n        {{\n            "command": "original command",\n            "steps": [\n                {{\n                    "action": "action_type",\n                    "parameters": {{"param1": "value1", ...}},\n                    "description": "brief description"\n                }}\n            ],\n            "estimated_duration": "estimated time in seconds"\n        }}\n\n        Action types: \'navigate\', \'grasp\', \'place\', \'speak\', \'gesture\', \'wait\', \'detect_object\'\n        """\n\n        user_prompt = f"Command: {command}"\n\n        response = self.client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": user_prompt}\n            ],\n            temperature=0.1,\n            max_tokens=500\n        )\n\n        # Parse the response\n        response_text = response.choices[0].message.content\n\n        # Extract JSON from response\n        try:\n            # Look for JSON in response\n            start_idx = response_text.find(\'{\')\n            end_idx = response_text.rfind(\'}\') + 1\n            json_str = response_text[start_idx:end_idx]\n\n            plan = json.loads(json_str)\n            return plan\n        except json.JSONDecodeError:\n            self.get_logger().error(f\'Could not parse LLM response as JSON: {response_text}\')\n            return self.generate_fallback_plan(command)\n\n    def generate_fallback_plan(self, command):\n        """Generate a simple fallback plan if LLM fails"""\n        # Simple rule-based fallback\n        if "bring me" in command or "get me" in command:\n            obj = command.split("bring me")[-1].split("get me")[-1].strip()\n            return {\n                "command": command,\n                "steps": [\n                    {"action": "detect_object", "parameters": {"object": obj}, "description": f"Locate {obj}"},\n                    {"action": "navigate", "parameters": {"target": "location_of_object"}, "description": f"Go to {obj}"},\n                    {"action": "grasp", "parameters": {"object": obj}, "description": f"Pick up {obj}"},\n                    {"action": "navigate", "parameters": {"target": "user_location"}, "description": "Return to user"},\n                    {"action": "place", "parameters": {"location": "in_front_of_user"}, "description": "Place object for user"}\n                ],\n                "estimated_duration": 120\n            }\n        else:\n            return {\n                "command": command,\n                "steps": [\n                    {"action": "speak", "parameters": {"text": f"I don\'t know how to {command}"}, "description": "Express inability to perform command"}\n                ],\n                "estimated_duration": 5\n            }\n\n    def publish_plan(self, plan):\n        """Publish the generated plan"""\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan)\n        self.plan_pub.publish(plan_msg)\n\n        # Execute the plan\n        self.execute_plan(plan)\n\n    def execute_plan(self, plan):\n        """Execute the plan steps"""\n        for step in plan[\'steps\']:\n            self.get_logger().info(f\'Executing: {step["description"]}\')\n\n            # Publish individual commands\n            cmd_msg = String()\n            cmd_msg.data = json.dumps(step)\n            self.command_pub.publish(cmd_msg)\n\n            # Simple delay - in real system, wait for completion\n            time.sleep(2)\n\n    def handle_planning_error(self, command, error_msg):\n        """Handle planning errors"""\n        error_response = {\n            "command": command,\n            "error": error_msg,\n            "suggestion": "Please rephrase your command or provide more specific instructions"\n        }\n\n        error_msg = String()\n        error_msg.data = json.dumps(error_response)\n        self.plan_pub.publish(error_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlanningNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"multimodal-reasoning-with-vision",children:"Multimodal Reasoning with Vision"}),"\n",(0,i.jsx)(e.h3,{id:"vision-language-integration",children:"Vision-Language Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import cv2\nimport numpy as np\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\n\nclass MultimodalLLMPlanner:\n    def __init__(self, node):\n        self.node = node\n        self.bridge = CvBridge()\n\n        # Subscribe to camera feed\n        self.image_sub = node.create_subscription(\n            Image,\n            \'/camera/rgb/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.latest_image = None\n        self.image_timestamp = None\n\n    def image_callback(self, msg):\n        """Process incoming image"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n            self.latest_image = cv_image\n            self.image_timestamp = msg.header.stamp\n        except Exception as e:\n            self.node.get_logger().error(f\'Image conversion error: {e}\')\n\n    def generate_vision_guided_plan(self, command, image=None):\n        """Generate plan with visual context"""\n        if image is None:\n            image = self.latest_image\n\n        if image is not None:\n            # In practice, use multimodal models that can process both text and images\n            # For now, we\'ll describe the image content\n            image_description = self.describe_image_content(image)\n\n            system_prompt = f"""\n            You are a planning assistant for a humanoid robot. The robot has the following capabilities: {self.node.robot_context[\'capabilities\']}.\n            The current visual scene shows: {image_description}\n            The environment contains these locations: {self.node.robot_context[\'environment\'][\'locations\']} and objects: {self.node.robot_context[\'environment\'][\'objects\']}.\n\n            Use the visual information to help interpret the command and generate a more accurate plan.\n            """\n        else:\n            system_prompt = f"""\n            You are a planning assistant for a humanoid robot. The robot has the following capabilities: {self.node.robot_context[\'capabilities\']}.\n            The environment contains these locations: {self.node.robot_context[\'environment\'][\'locations\']} and objects: {self.node.robot_context[\'environment\'][\'objects\']}.\n            """\n\n        user_prompt = f"Command: {command}"\n\n        # Call LLM with multimodal context\n        response = self.node.client.chat.completions.create(\n            model="gpt-4-vision-preview",  # Use vision-capable model if available\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": user_prompt}\n            ],\n            temperature=0.1,\n            max_tokens=500\n        )\n\n        # Process response similar to basic LLM integration\n        response_text = response.choices[0].message.content\n        # Extract and return plan...\n\n        return self.parse_plan_response(response_text)\n\n    def describe_image_content(self, image):\n        """Simple image description (in practice, use computer vision models)"""\n        # This is a simplified example\n        # In practice, use object detection, scene understanding, etc.\n        height, width, channels = image.shape\n\n        # Example: detect if image is indoor/outdoor, approximate number of objects, etc.\n        return f"Image is {width}x{height} pixels with {channels} channels. Scene appears to be indoor with multiple objects visible."\n'})}),"\n",(0,i.jsx)(e.h2,{id:"context-aware-planning",children:"Context-Aware Planning"}),"\n",(0,i.jsx)(e.h3,{id:"maintaining-planning-context",children:"Maintaining Planning Context"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class ContextAwarePlanner:\n    def __init__(self, node):\n        self.node = node\n        self.context_history = []\n        self.current_plan = None\n        self.execution_state = "idle"\n\n    def update_context(self, observation):\n        """Update planning context with new observations"""\n        context_entry = {\n            "timestamp": self.node.get_clock().now().to_msg(),\n            "observation": observation,\n            "robot_state": self.get_robot_state(),\n            "environment_state": self.get_environment_state()\n        }\n\n        self.context_history.append(context_entry)\n\n        # Keep only recent context (last 10 entries)\n        if len(self.context_history) > 10:\n            self.context_history = self.context_history[-10:]\n\n    def get_robot_state(self):\n        """Get current robot state"""\n        # In practice, this would come from robot state publisher\n        return {\n            "location": "current_location",\n            "battery": 85,\n            "carrying": None,\n            "executing_task": bool(self.current_plan)\n        }\n\n    def get_environment_state(self):\n        """Get current environment state"""\n        # In practice, this would come from perception system\n        return {\n            "known_objects": ["cup", "book"],\n            "free_space": True,\n            "obstacles": []\n        }\n\n    def adapt_plan_to_context(self, original_plan, context):\n        """Adapt plan based on current context"""\n        # Check if original plan is still valid given current context\n        if not self.is_plan_still_valid(original_plan, context):\n            # Regenerate plan with new context\n            return self.regenerate_plan_with_context(original_plan, context)\n\n        return original_plan\n\n    def is_plan_still_valid(self, plan, context):\n        """Check if plan is still valid given current context"""\n        # Check if environment has changed significantly\n        # Check if robot state allows plan execution\n        # Check if goal is still relevant\n\n        # Simple example: if carrying object changed\n        if context["robot_state"]["carrying"] != plan.get("required_carrying_state"):\n            return False\n\n        return True\n'})}),"\n",(0,i.jsx)(e.h2,{id:"advanced-planning-strategies",children:"Advanced Planning Strategies"}),"\n",(0,i.jsx)(e.h3,{id:"hierarchical-task-planning",children:"Hierarchical Task Planning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class HierarchicalPlanner:\n    def __init__(self, node):\n        self.node = node\n        self.task_hierarchy = {}\n\n    def create_hierarchical_plan(self, high_level_command):\n        """Create hierarchical plan with multiple levels of abstraction"""\n        # High-level plan\n        high_level_plan = self.generate_high_level_plan(high_level_command)\n\n        # Decompose into subtasks\n        detailed_plan = self.decompose_into_subtasks(high_level_plan)\n\n        # Add execution details\n        executable_plan = self.add_execution_details(detailed_plan)\n\n        return executable_plan\n\n    def generate_high_level_plan(self, command):\n        """Generate high-level task decomposition"""\n        system_prompt = """\n        Decompose the given command into high-level tasks that can be executed by a humanoid robot.\n        Each task should be meaningful and achievable.\n        Return in JSON format with \'tasks\' array.\n        """\n\n        user_prompt = f"Decompose this command: {command}"\n\n        response = self.node.client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": user_prompt}\n            ],\n            temperature=0.1,\n            max_tokens=300\n        )\n\n        return json.loads(response.choices[0].message.content)\n\n    def decompose_into_subtasks(self, high_level_plan):\n        """Decompose high-level tasks into executable subtasks"""\n        detailed_plan = {"tasks": []}\n\n        for task in high_level_plan["tasks"]:\n            subtasks = self.generate_subtasks_for_task(task)\n            detailed_plan["tasks"].append({\n                "task": task,\n                "subtasks": subtasks\n            })\n\n        return detailed_plan\n\n    def generate_subtasks_for_task(self, task):\n        """Generate executable subtasks for a high-level task"""\n        # Use LLM to generate subtasks\n        system_prompt = """\n        For the given high-level task, generate specific executable subtasks that a humanoid robot can perform.\n        Each subtask should be simple and specific.\n        """\n\n        user_prompt = f"Generate subtasks for: {task}"\n\n        response = self.node.client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": user_prompt}\n            ],\n            temperature=0.1,\n            max_tokens=300\n        )\n\n        return json.loads(response.choices[0].message.content)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,i.jsx)(e.h3,{id:"plan-validation-framework",children:"Plan Validation Framework"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class PlanValidator:\n    def __init__(self, node):\n        self.node = node\n\n    def validate_plan(self, plan):\n        """Validate plan for safety and feasibility"""\n        validation_results = {\n            "is_safe": True,\n            "is_feasible": True,\n            "issues": [],\n            "suggestions": []\n        }\n\n        # Check safety constraints\n        safety_check = self.check_safety_constraints(plan)\n        if not safety_check["passed"]:\n            validation_results["is_safe"] = False\n            validation_results["issues"].extend(safety_check["issues"])\n            validation_results["suggestions"].extend(safety_check["suggestions"])\n\n        # Check feasibility\n        feasibility_check = self.check_feasibility(plan)\n        if not feasibility_check["passed"]:\n            validation_results["is_feasible"] = False\n            validation_results["issues"].extend(feasibility_check["issues"])\n            validation_results["suggestions"].extend(feasibility_check["suggestions"])\n\n        return validation_results\n\n    def check_safety_constraints(self, plan):\n        """Check if plan violates safety constraints"""\n        issues = []\n        suggestions = []\n\n        for step in plan.get("steps", []):\n            action = step.get("action", "")\n            params = step.get("parameters", {})\n\n            # Check navigation safety\n            if action == "navigate":\n                target = params.get("target", "")\n                if self.is_unsafe_location(target):\n                    issues.append(f"Navigation to {target} may be unsafe")\n                    suggestions.append(f"Find alternative route to {target}")\n\n            # Check manipulation safety\n            elif action == "grasp":\n                obj = params.get("object", "")\n                if self.is_hazardous_object(obj):\n                    issues.append(f"Grasping {obj} may be hazardous")\n                    suggestions.append(f"Avoid grasping {obj} or use protective measures")\n\n        return {\n            "passed": len(issues) == 0,\n            "issues": issues,\n            "suggestions": suggestions\n        }\n\n    def is_unsafe_location(self, location):\n        """Check if location is unsafe"""\n        # In practice, this would check against known unsafe locations\n        unsafe_locations = ["construction_zone", "restricted_area"]\n        return location in unsafe_locations\n\n    def is_hazardous_object(self, obj):\n        """Check if object is hazardous"""\n        # In practice, this would check against object database\n        hazardous_objects = ["knife", "chemical", "hot_item"]\n        return obj.lower() in [hazard.lower() for hazard in hazardous_objects]\n\n    def check_feasibility(self, plan):\n        """Check if plan is physically feasible"""\n        issues = []\n        suggestions = []\n\n        # Check if robot can perform required actions\n        for step in plan.get("steps", []):\n            action = step.get("action", "")\n            if not self.is_action_feasible(action):\n                issues.append(f"Action {action} is not feasible for this robot")\n                suggestions.append(f"Replace {action} with alternative action")\n\n        return {\n            "passed": len(issues) == 0,\n            "issues": issues,\n            "suggestions": suggestions\n        }\n\n    def is_action_feasible(self, action):\n        """Check if action is feasible for robot"""\n        feasible_actions = [\n            "navigate", "grasp", "place", "speak", "gesture",\n            "detect_object", "wait", "approach", "avoid"\n        ]\n        return action in feasible_actions\n'})}),"\n",(0,i.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(e.h3,{id:"caching-and-pre-planning",children:"Caching and Pre-planning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import hashlib\nfrom functools import lru_cache\n\nclass OptimizedLLMPlanner:\n    def __init__(self, node):\n        self.node = node\n        self.plan_cache = {}\n        self.max_cache_size = 100\n\n    @lru_cache(maxsize=100)\n    def get_cached_plan(self, command_hash):\n        """Get cached plan for command hash"""\n        # This is called by the decorator\n        pass\n\n    def generate_plan_with_caching(self, command):\n        """Generate plan with caching to improve performance"""\n        # Create hash of command for caching\n        command_hash = hashlib.md5(command.encode()).hexdigest()\n\n        # Check cache first\n        if command_hash in self.plan_cache:\n            self.node.get_logger().info("Using cached plan")\n            return self.plan_cache[command_hash]\n\n        # Generate new plan\n        plan = self.generate_plan_with_llm(command)\n\n        # Cache the plan\n        if len(self.plan_cache) >= self.max_cache_size:\n            # Remove oldest entry\n            oldest_key = next(iter(self.plan_cache))\n            del self.plan_cache[oldest_key]\n\n        self.plan_cache[command_hash] = plan\n\n        return plan\n'})}),"\n",(0,i.jsx)(e.h2,{id:"exercise",children:"Exercise"}),"\n",(0,i.jsx)(e.p,{children:"Create an LLM cognitive planning system that:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Integrates with OpenAI or similar LLM service"}),"\n",(0,i.jsx)(e.li,{children:"Generates detailed execution plans for complex commands"}),"\n",(0,i.jsx)(e.li,{children:"Incorporates visual context from camera feeds"}),"\n",(0,i.jsx)(e.li,{children:"Implements plan validation for safety and feasibility"}),"\n",(0,i.jsx)(e.li,{children:"Handles plan adaptation when context changes"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"LLM cognitive planning provides sophisticated task decomposition and execution planning for humanoid robots. By combining natural language understanding with contextual awareness, robots can execute complex, multi-step commands. Proper validation and safety checking ensure reliable operation. The next lesson will cover the capstone project integrating all modules."})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>o,x:()=>l});var a=t(6540);const i={},s=a.createContext(i);function o(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:o(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);