"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[6064],{9571(i){i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Introduction","items":[{"type":"link","href":"/physical-ai-humanoid-robotics/docs/intro","label":"Introduction","docId":"intro","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-1/","label":"Module 1 - The Robotic Nervous System (ROS 2)","docId":"module-1/index","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-1/ros-nodes","label":"ROS Nodes","docId":"module-1/ros-nodes","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-1/topics-services","label":"Topics and Services","docId":"module-1/topics-services","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-1/rclpy-bridge","label":"rclpy Bridge","docId":"module-1/rclpy-bridge","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-1/urdf-humanoids","label":"URDF for Humanoids","docId":"module-1/urdf-humanoids","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-2/","label":"Module 2 - The Digital Twin (Gazebo & Unity)","docId":"module-2/index","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-2/physics-simulation","label":"Physics Simulation","docId":"module-2/physics-simulation","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-2/collisions","label":"Collisions","docId":"module-2/collisions","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-2/rendering","label":"Rendering","docId":"module-2/rendering","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-2/sensors","label":"Sensors","docId":"module-2/sensors","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-3/","label":"Module 3 - The AI-Robot Brain (NVIDIA Isaac\u2122)","docId":"module-3/index","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-3/isaac-sim","label":"Isaac Sim","docId":"module-3/isaac-sim","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-3/vslam-navigation","label":"VSLAM Navigation","docId":"module-3/vslam-navigation","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-3/nav2-bipedal","label":"Nav2 Bipedal Planning","docId":"module-3/nav2-bipedal","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-4/","label":"Module 4 - Vision-Language-Action (VLA)","docId":"module-4/index","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-4/voice-to-action","label":"Voice-to-Action","docId":"module-4/voice-to-action","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-4/llm-planning","label":"LLM Cognitive Planning","docId":"module-4/llm-planning","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/module-4/capstone-project","label":"Capstone Project","docId":"module-4/capstone-project","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Appendix","items":[{"type":"link","href":"/physical-ai-humanoid-robotics/docs/appendix/troubleshooting","label":"Troubleshooting Guide","docId":"appendix/troubleshooting","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/appendix/faq","label":"Frequently Asked Questions","docId":"appendix/faq","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics/docs/appendix/glossary","label":"Glossary","docId":"appendix/glossary","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"appendix/faq":{"id":"appendix/faq","title":"Frequently Asked Questions","description":"This section addresses common questions about Physical AI & Humanoid Robotics development and deployment.","sidebar":"tutorialSidebar"},"appendix/glossary":{"id":"appendix/glossary","title":"Glossary","description":"This glossary provides definitions for key terms used throughout the Physical AI & Humanoid Robotics documentation.","sidebar":"tutorialSidebar"},"appendix/troubleshooting":{"id":"appendix/troubleshooting","title":"Troubleshooting Guide","description":"This guide provides solutions for common issues encountered when developing and operating Physical AI & Humanoid Robotics systems.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction","description":"Welcome to the comprehensive guide on Physical AI & Humanoid Robotics: Embodied Intelligence. This course bridges the gap between digital AI and physical bodies, teaching you how to design, simulate, and deploy intelligent humanoid robots with natural interactions using ROS 2, Gazebo, and NVIDIA Isaac.","sidebar":"tutorialSidebar"},"module-1/index":{"id":"module-1/index","title":"Module 1 - The Robotic Nervous System (ROS 2)","description":"Welcome to Module 1 of Physical AI & Humanoid Robotics: Embodied Intelligence. In this module, you\'ll learn about the Robot Operating System (ROS 2), which serves as the nervous system for robotic applications.","sidebar":"tutorialSidebar"},"module-1/rclpy-bridge":{"id":"module-1/rclpy-bridge","title":"rclpy Bridge","description":"The rclpy package is the Python client library for ROS 2. It provides the Python API for developing ROS 2 packages and enables Python developers to create ROS 2 nodes, publishers, subscribers, services, and more.","sidebar":"tutorialSidebar"},"module-1/ros-nodes":{"id":"module-1/ros-nodes","title":"ROS Nodes","description":"In ROS 2, a node is a process that performs computation. Nodes are the fundamental building blocks of a ROS 2 system. They communicate with other nodes using topics, services, actions, and parameters.","sidebar":"tutorialSidebar"},"module-1/topics-services":{"id":"module-1/topics-services","title":"Topics and Services","description":"ROS 2 provides two primary communication patterns: topics (for asynchronous, broadcast communication) and services (for synchronous, request-response communication). Understanding these patterns is crucial for designing effective robotic systems.","sidebar":"tutorialSidebar"},"module-1/urdf-humanoids":{"id":"module-1/urdf-humanoids","title":"URDF for Humanoids","description":"Unified Robot Description Format (URDF) is an XML format for representing a robot model. URDF is used in ROS to describe the physical and kinematic properties of a robot, including its joints, links, and visual/inertial properties. For humanoid robots, URDF is crucial for simulation, visualization, and kinematic calculations.","sidebar":"tutorialSidebar"},"module-2/collisions":{"id":"module-2/collisions","title":"Collisions","description":"Collision detection is a critical component of physics simulation for humanoid robots, ensuring realistic interactions with the environment and preventing parts of the robot from passing through obstacles or itself.","sidebar":"tutorialSidebar"},"module-2/index":{"id":"module-2/index","title":"Module 2 - The Digital Twin (Gazebo & Unity)","description":"Welcome to Module 2 of Physical AI & Humanoid Robotics: Embodied Intelligence. In this module, you\'ll learn about creating digital twins for humanoid robots using Gazebo and Unity, including physics simulation, collision detection, and sensor integration.","sidebar":"tutorialSidebar"},"module-2/physics-simulation":{"id":"module-2/physics-simulation","title":"Physics Simulation","description":"Physics simulation is a crucial component of digital twins for humanoid robotics. It enables accurate modeling of physical interactions, forces, and movements before deployment to real hardware.","sidebar":"tutorialSidebar"},"module-2/rendering":{"id":"module-2/rendering","title":"Rendering","description":"Rendering in simulation environments provides visual feedback for robot behavior, enables human-in-the-loop interaction, and supports debugging and visualization of complex humanoid robot behaviors.","sidebar":"tutorialSidebar"},"module-2/sensors":{"id":"module-2/sensors","title":"Sensors","description":"Sensors are crucial components of digital twins, providing the data needed for perception, navigation, and control in humanoid robotics applications. This lesson covers the integration of various sensor types in simulation environments.","sidebar":"tutorialSidebar"},"module-3/index":{"id":"module-3/index","title":"Module 3 - The AI-Robot Brain (NVIDIA Isaac\u2122)","description":"Welcome to Module 3 of Physical AI & Humanoid Robotics: Embodied Intelligence. In this module, you\'ll explore NVIDIA Isaac for creating intelligent robot behaviors, including visual SLAM, navigation, and bipedal path planning.","sidebar":"tutorialSidebar"},"module-3/isaac-sim":{"id":"module-3/isaac-sim","title":"Isaac Sim","description":"Isaac Sim is NVIDIA\'s high-fidelity simulation environment built on the Omniverse platform. It provides photorealistic rendering, accurate physics simulation, and tools for generating synthetic data to train AI models for robotics applications.","sidebar":"tutorialSidebar"},"module-3/nav2-bipedal":{"id":"module-3/nav2-bipedal","title":"Nav2 Bipedal Planning","description":"Navigation 2 (Nav2) is the ROS 2 navigation stack that provides path planning and navigation capabilities. For humanoid robots, Nav2 requires special configuration to handle bipedal locomotion patterns and unique mobility constraints.","sidebar":"tutorialSidebar"},"module-3/vslam-navigation":{"id":"module-3/vslam-navigation","title":"VSLAM Navigation","description":"Visual Simultaneous Localization and Mapping (VSLAM) is a critical technology for humanoid robots to understand and navigate their environment using visual sensors. Isaac ROS provides optimized implementations for VSLAM and navigation tasks.","sidebar":"tutorialSidebar"},"module-4/capstone-project":{"id":"module-4/capstone-project","title":"Capstone Project","description":"The capstone project integrates all modules to create an autonomous humanoid robot capable of receiving voice commands, understanding them cognitively, and executing appropriate actions in real-world environments.","sidebar":"tutorialSidebar"},"module-4/index":{"id":"module-4/index","title":"Module 4 - Vision-Language-Action (VLA)","description":"Welcome to Module 4 of Physical AI & Humanoid Robotics: Embodied Intelligence. In this module, you\'ll learn about implementing voice-controlled humanoid robots using OpenAI Whisper and LLM cognitive planning for natural language commands.","sidebar":"tutorialSidebar"},"module-4/llm-planning":{"id":"module-4/llm-planning","title":"LLM Cognitive Planning","description":"Large Language Models (LLMs) provide cognitive planning capabilities that enable humanoid robots to understand complex natural language commands and generate appropriate action sequences. This lesson covers the integration of LLMs for robotic task planning.","sidebar":"tutorialSidebar"},"module-4/voice-to-action":{"id":"module-4/voice-to-action","title":"Voice-to-Action","description":"Voice-to-action systems enable humanoid robots to understand spoken commands and translate them into robotic actions. This technology provides a natural interface for human-robot interaction.","sidebar":"tutorialSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template."},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed..."},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack)."},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features."},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs."},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French."}}}}')}}]);