"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[929],{4931(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/capstone-project","title":"Capstone Project","description":"The capstone project integrates all modules to create an autonomous humanoid robot capable of receiving voice commands, understanding them cognitively, and executing appropriate actions in real-world environments.","source":"@site/docs/module-4/capstone-project.md","sourceDirName":"module-4","slug":"/module-4/capstone-project","permalink":"/physical-ai-humanoid-robotics/docs/module-4/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-humanoid-robotics/edit/main/docusaurus/docs/module-4/capstone-project.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Capstone Project"},"sidebar":"tutorialSidebar","previous":{"title":"LLM Cognitive Planning","permalink":"/physical-ai-humanoid-robotics/docs/module-4/llm-planning"},"next":{"title":"Troubleshooting Guide","permalink":"/physical-ai-humanoid-robotics/docs/appendix/troubleshooting"}}');var i=s(4848),o=s(8453);const r={sidebar_position:4,title:"Capstone Project"},a="Capstone Project: Autonomous Humanoid System",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:3},{value:"ROS 2 Package Structure",id:"ros-2-package-structure",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: System Integration Node",id:"step-1-system-integration-node",level:3},{value:"Step 2: Launch File Integration",id:"step-2-launch-file-integration",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Test Scenarios",id:"test-scenarios",level:3},{value:"Scenario 1: Simple Navigation",id:"scenario-1-simple-navigation",level:4},{value:"Scenario 2: Object Retrieval",id:"scenario-2-object-retrieval",level:4},{value:"Scenario 3: Multi-step Interaction",id:"scenario-3-multi-step-interaction",level:4},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"Real Robot Deployment",id:"real-robot-deployment",level:3},{value:"Simulation Deployment",id:"simulation-deployment",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Voice Recognition Issues",id:"voice-recognition-issues",level:3},{value:"Planning Failures",id:"planning-failures",level:3},{value:"Navigation Failures",id:"navigation-failures",level:3},{value:"Execution Failures",id:"execution-failures",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"Success Metrics",id:"success-metrics",level:3},{value:"Assessment Rubric",id:"assessment-rubric",level:3},{value:"Future Enhancements",id:"future-enhancements",level:2},{value:"Advanced Features",id:"advanced-features",level:3},{value:"Research Directions",id:"research-directions",level:3},{value:"Exercise",id:"exercise",level:2},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capstone-project-autonomous-humanoid-system",children:"Capstone Project: Autonomous Humanoid System"})}),"\n",(0,i.jsx)(n.p,{children:"The capstone project integrates all modules to create an autonomous humanoid robot capable of receiving voice commands, understanding them cognitively, and executing appropriate actions in real-world environments."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"After completing this capstone project, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate all four modules into a cohesive humanoid robot system"}),"\n",(0,i.jsx)(n.li,{children:"Implement end-to-end voice command processing from speech to action"}),"\n",(0,i.jsx)(n.li,{children:"Deploy and test the complete autonomous humanoid system"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate system performance and identify improvement areas"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsx)(n.p,{children:"The capstone project combines:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 1"}),": ROS 2 communication and humanoid control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 2"}),": Digital twin simulation and sensor integration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 3"}),": AI perception and navigation systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 4"}),": Voice-to-action and cognitive planning"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'The final system will accept natural language commands like "Please bring me the red cup from the kitchen" and execute them autonomously.'}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph TB\n    A[Voice Command] --\x3e B[Speech Recognition]\n    B --\x3e C[LLM Cognitive Planning]\n    C --\x3e D[Task Decomposition]\n    D --\x3e E[Navigation System]\n    D --\x3e F[Manipulation System]\n    D --\x3e G[Communication System]\n\n    H[Sensor Fusion] --\x3e I[Perception System]\n    I --\x3e E\n    I --\x3e F\n\n    E --\x3e J[Humanoid Robot]\n    F --\x3e J\n    G --\x3e J\n\n    J --\x3e K[Environment]\n    K --\x3e H\n    K --\x3e B"}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-package-structure",children:"ROS 2 Package Structure"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"humanoid_robot_system/\n\u251c\u2500\u2500 voice_to_action/          # Module 4: Speech recognition\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 launch/\n\u2502   \u2514\u2500\u2500 config/\n\u251c\u2500\u2500 cognitive_planning/       # Module 4: LLM integration\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 launch/\n\u2502   \u2514\u2500\u2500 config/\n\u251c\u2500\u2500 navigation_system/        # Module 3: Isaac ROS Nav2\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 launch/\n\u2502   \u2514\u2500\u2500 config/\n\u251c\u2500\u2500 perception_system/        # Module 2: Sensor integration\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 launch/\n\u2502   \u2514\u2500\u2500 config/\n\u251c\u2500\u2500 robot_control/            # Module 1: ROS 2 nodes\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 launch/\n\u2502   \u2514\u2500\u2500 config/\n\u2514\u2500\u2500 system_integration/       # Capstone integration\n    \u251c\u2500\u2500 src/\n    \u251c\u2500\u2500 launch/\n    \u2514\u2500\u2500 config/\n"})}),"\n",(0,i.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-system-integration-node",children:"Step 1: System Integration Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import JointState\nimport json\nimport threading\nimport time\n\nclass HumanoidSystemIntegrator(Node):\n    def __init__(self):\n        super().__init__('humanoid_system_integrator')\n\n        # Publishers for different subsystems\n        self.voice_cmd_pub = self.create_publisher(String, '/voice_commands', 10)\n        self.nav_goal_pub = self.create_publisher(Pose, '/navigation/goal', 10)\n        self.manip_cmd_pub = self.create_publisher(String, '/manipulation/command', 10)\n        self.robot_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)\n\n        # Subscribers for system status\n        self.voice_sub = self.create_subscription(\n            String, '/voice_recognition/output', self.voice_callback, 10\n        )\n        self.planning_sub = self.create_subscription(\n            String, '/cognitive_planning/output', self.planning_callback, 10\n        )\n        self.nav_status_sub = self.create_subscription(\n            String, '/navigation/status', self.nav_status_callback, 10\n        )\n        self.robot_status_sub = self.create_subscription(\n            String, '/robot/status', self.robot_status_callback, 10\n        )\n\n        # System state\n        self.system_state = {\n            'current_task': None,\n            'task_queue': [],\n            'robot_status': 'idle',\n            'navigation_status': 'idle',\n            'voice_status': 'listening'\n        }\n\n        # Timer for system monitoring\n        self.monitor_timer = self.create_timer(1.0, self.system_monitor)\n\n    def voice_callback(self, msg):\n        \"\"\"Handle recognized voice commands\"\"\"\n        try:\n            command_data = json.loads(msg.data)\n            command = command_data.get('command', '')\n\n            self.get_logger().info(f'Processing voice command: {command}')\n\n            # Send command to cognitive planning\n            plan_msg = String()\n            plan_msg.data = json.dumps({'command': command, 'context': self.get_system_context()})\n            self.voice_cmd_pub.publish(plan_msg)\n\n        except json.JSONDecodeError:\n            # Handle simple text commands\n            self.get_logger().info(f'Processing simple command: {msg.data}')\n\n            # Send to planning system\n            plan_msg = String()\n            plan_msg.data = json.dumps({'command': msg.data, 'context': self.get_system_context()})\n            self.voice_cmd_pub.publish(plan_msg)\n\n    def planning_callback(self, msg):\n        \"\"\"Handle cognitive planning output\"\"\"\n        try:\n            plan = json.loads(msg.data)\n\n            if plan.get('success', False):\n                self.get_logger().info('Plan received, executing...')\n                self.execute_plan(plan)\n            else:\n                self.get_logger().error(f'Planning failed: {plan.get(\"error\", \"Unknown error\")}')\n                self.handle_planning_failure(plan)\n\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid plan format received')\n\n    def execute_plan(self, plan):\n        \"\"\"Execute the received plan\"\"\"\n        self.system_state['current_task'] = plan\n\n        for step in plan.get('steps', []):\n            self.execute_step(step)\n\n            # Wait for step completion\n            self.wait_for_step_completion(step)\n\n    def execute_step(self, step):\n        \"\"\"Execute a single step of the plan\"\"\"\n        action = step.get('action', '')\n        params = step.get('parameters', {})\n\n        if action == 'navigate':\n            self.execute_navigation_step(params)\n        elif action == 'grasp':\n            self.execute_grasp_step(params)\n        elif action == 'place':\n            self.execute_place_step(params)\n        elif action == 'speak':\n            self.execute_speak_step(params)\n        else:\n            self.get_logger().warn(f'Unknown action: {action}')\n\n    def execute_navigation_step(self, params):\n        \"\"\"Execute navigation step\"\"\"\n        target = params.get('target', '')\n\n        # In a real system, this would call navigation services\n        goal_msg = Pose()\n        # Set goal based on target location\n        self.nav_goal_pub.publish(goal_msg)\n\n        self.get_logger().info(f'Navigating to: {target}')\n\n    def execute_grasp_step(self, params):\n        \"\"\"Execute grasp step\"\"\"\n        obj = params.get('object', '')\n\n        # Publish manipulation command\n        cmd_msg = String()\n        cmd_msg.data = json.dumps({'action': 'grasp', 'object': obj})\n        self.manip_cmd_pub.publish(cmd_msg)\n\n        self.get_logger().info(f'Grasping object: {obj}')\n\n    def system_monitor(self):\n        \"\"\"Monitor overall system status\"\"\"\n        status_msg = String()\n        status_msg.data = json.dumps(self.system_state)\n\n        # Publish system status\n        self.create_publisher(String, '/system/status', 10).publish(status_msg)\n\n    def get_system_context(self):\n        \"\"\"Get current system context for planning\"\"\"\n        return {\n            'robot_location': 'current_location',  # Would come from localization\n            'battery_level': 85,  # Would come from robot status\n            'environment_objects': ['cup', 'book', 'phone'],  # Would come from perception\n            'robot_capabilities': ['navigation', 'manipulation', 'speech']\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HumanoidSystemIntegrator()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-launch-file-integration",children:"Step 2: Launch File Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"\x3c!-- system_integration/launch/capstone_system.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time'\n        ),\n\n        # Voice-to-action node\n        Node(\n            package='voice_to_action',\n            executable='voice_to_action_node',\n            name='voice_to_action_node',\n            parameters=[\n                {'use_sim_time': LaunchConfiguration('use_sim_time')}\n            ],\n            output='screen'\n        ),\n\n        # Cognitive planning node\n        Node(\n            package='cognitive_planning',\n            executable='llm_planning_node',\n            name='llm_planning_node',\n            parameters=[\n                {'use_sim_time': LaunchConfiguration('use_sim_time')}\n            ],\n            output='screen'\n        ),\n\n        # Navigation system\n        Node(\n            package='navigation_system',\n            executable='nav2_system',\n            name='nav2_system',\n            parameters=[\n                {'use_sim_time': LaunchConfiguration('use_sim_time')}\n            ],\n            output='screen'\n        ),\n\n        # Perception system\n        Node(\n            package='perception_system',\n            executable='perception_node',\n            name='perception_node',\n            parameters=[\n                {'use_sim_time': LaunchConfiguration('use_sim_time')}\n            ],\n            output='screen'\n        ),\n\n        # Robot control system\n        Node(\n            package='robot_control',\n            executable='robot_controller',\n            name='robot_controller',\n            parameters=[\n                {'use_sim_time': LaunchConfiguration('use_sim_time')}\n            ],\n            output='screen'\n        ),\n\n        # System integrator (capstone)\n        Node(\n            package='system_integration',\n            executable='system_integrator',\n            name='system_integrator',\n            parameters=[\n                {'use_sim_time': LaunchConfiguration('use_sim_time')}\n            ],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,i.jsx)(n.h3,{id:"test-scenarios",children:"Test Scenarios"}),"\n",(0,i.jsx)(n.h4,{id:"scenario-1-simple-navigation",children:"Scenario 1: Simple Navigation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Command"}),': "Go to the kitchen"\n',(0,i.jsx)(n.strong,{children:"Expected behavior"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Voice recognition detects command"}),"\n",(0,i.jsx)(n.li,{children:"LLM planning generates navigation plan"}),"\n",(0,i.jsx)(n.li,{children:"Navigation system executes path to kitchen"}),"\n",(0,i.jsx)(n.li,{children:"Robot confirms arrival"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"scenario-2-object-retrieval",children:"Scenario 2: Object Retrieval"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Command"}),': "Please bring me the red cup from the kitchen"\n',(0,i.jsx)(n.strong,{children:"Expected behavior"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Voice recognition processes complex command"}),"\n",(0,i.jsx)(n.li,{children:"LLM decomposes into: navigate \u2192 detect object \u2192 grasp \u2192 return"}),"\n",(0,i.jsx)(n.li,{children:"Perception system identifies red cup"}),"\n",(0,i.jsx)(n.li,{children:"Navigation system moves to kitchen"}),"\n",(0,i.jsx)(n.li,{children:"Manipulation system grasps cup"}),"\n",(0,i.jsx)(n.li,{children:"Navigation system returns to user"}),"\n",(0,i.jsx)(n.li,{children:"Manipulation system places cup"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"scenario-3-multi-step-interaction",children:"Scenario 3: Multi-step Interaction"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Command"}),': "After you bring me the cup, please tell me the time"\n',(0,i.jsx)(n.strong,{children:"Expected behavior"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Processes compound command"}),"\n",(0,i.jsx)(n.li,{children:"Executes object retrieval first"}),"\n",(0,i.jsx)(n.li,{children:"Then executes speech action"}),"\n",(0,i.jsx)(n.li,{children:"Handles sequence properly"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32\nimport time\n\nclass SystemPerformanceMonitor(Node):\n    def __init__(self):\n        super().__init__('system_performance_monitor')\n\n        self.metrics = {\n            'command_accuracy': 0.0,\n            'execution_success_rate': 0.0,\n            'response_time': 0.0,\n            'task_completion_time': 0.0\n        }\n\n        # Publishers for metrics\n        self.accuracy_pub = self.create_publisher(Float32, '/metrics/command_accuracy', 10)\n        self.success_rate_pub = self.create_publisher(Float32, '/metrics/execution_success_rate', 10)\n        self.response_time_pub = self.create_publisher(Float32, '/metrics/response_time', 10)\n\n        # Timer for metric calculation\n        self.metric_timer = self.create_timer(5.0, self.calculate_metrics)\n\n    def calculate_metrics(self):\n        \"\"\"Calculate and publish system metrics\"\"\"\n        # Calculate metrics based on system logs\n        # This would involve analyzing command history, execution results, etc.\n\n        accuracy_msg = Float32()\n        accuracy_msg.data = self.metrics['command_accuracy']\n        self.accuracy_pub.publish(accuracy_msg)\n\n        success_msg = Float32()\n        success_msg.data = self.metrics['execution_success_rate']\n        self.success_rate_pub.publish(success_msg)\n\n        response_msg = Float32()\n        response_msg.data = self.metrics['response_time']\n        self.response_time_pub.publish(response_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SystemPerformanceMonitor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"real-robot-deployment",children:"Real Robot Deployment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# deployment/config/real_robot.yaml\nsystem_config:\n  use_sim_time: false\n  robot_hardware:\n    joint_state_topic: "/joint_states"\n    cmd_vel_topic: "/cmd_vel"\n    camera_topic: "/camera/rgb/image_raw"\n    lidar_topic: "/scan"\n\n  performance:\n    max_cpu_usage: 80\n    min_memory: 4096  # MB\n    network_timeout: 30  # seconds for LLM calls\n\n  safety:\n    emergency_stop_topic: "/emergency_stop"\n    safety_radius: 0.5  # meters\n    max_speed: 0.5  # m/s\n'})}),"\n",(0,i.jsx)(n.h3,{id:"simulation-deployment",children:"Simulation Deployment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# deployment/config/simulation.yaml\nsystem_config:\n  use_sim_time: true\n  simulation:\n    physics_rate: 1000  # Hz\n    rendering: true\n\n  performance:\n    max_cpu_usage: 90\n    min_memory: 2048\n    network_timeout: 5  # seconds (faster in simulation)\n\n  testing:\n    enable_detailed_logging: true\n    record_bag_files: true\n"})}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,i.jsx)(n.h3,{id:"voice-recognition-issues",children:"Voice Recognition Issues"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": Poor recognition in noisy environments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Use noise cancellation, directional microphones, or wake-word detection"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"planning-failures",children:"Planning Failures"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": LLM generates invalid plans"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Implement plan validation, use structured prompts, add error recovery"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"navigation-failures",children:"Navigation Failures"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": Robot gets stuck or takes inefficient paths"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Improve costmap configuration, add dynamic obstacle avoidance"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"execution-failures",children:"Execution Failures"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Problem"}),": Robot fails to execute planned actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Add feedback monitoring, implement action recovery, improve perception"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,i.jsx)(n.h3,{id:"success-metrics",children:"Success Metrics"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command Success Rate"}),": Percentage of commands executed successfully"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Response Time"}),": Average time from command to execution start"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Completion"}),": Percentage of multi-step tasks completed successfully"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness"}),": Ability to recover from errors and adapt to changes"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"assessment-rubric",children:"Assessment Rubric"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Excellent (A)"}),": >90% success rate, <10s response time, robust error handling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Good (B)"}),": 75-90% success rate, <15s response time, basic error recovery"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Satisfactory (C)"}),": 60-75% success rate, <20s response time, limited error handling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Needs Improvement (D)"}),": <60% success rate or significant reliability issues"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"future-enhancements",children:"Future Enhancements"}),"\n",(0,i.jsx)(n.h3,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Emotional Intelligence"}),": Recognize and respond to user emotions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Learning from Interaction"}),": Improve performance based on user feedback"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Collaborative Tasks"}),": Work with multiple robots or humans"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Extended Memory"}),": Remember previous interactions and preferences"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"research-directions",children:"Research Directions"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multimodal Learning"}),": Better integration of vision, language, and action"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Commonsense Reasoning"}),": More sophisticated understanding of the world"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adaptive Interfaces"}),": Adjust to different user capabilities and preferences"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"exercise",children:"Exercise"}),"\n",(0,i.jsx)(n.p,{children:"Implement the complete capstone system by:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Integrating all four modules into a single system"}),"\n",(0,i.jsx)(n.li,{children:"Creating launch files for coordinated startup"}),"\n",(0,i.jsx)(n.li,{children:"Implementing system monitoring and metrics"}),"\n",(0,i.jsx)(n.li,{children:"Testing with various command scenarios"}),"\n",(0,i.jsx)(n.li,{children:"Evaluating system performance and documenting results"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"The capstone project demonstrates the integration of all modules into a complete autonomous humanoid system. Success requires careful attention to system integration, error handling, and performance optimization. The resulting system represents a significant achievement in embodied AI, combining physical robotics with advanced cognitive capabilities."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453(e,n,s){s.d(n,{R:()=>r,x:()=>a});var t=s(6540);const i={},o=t.createContext(i);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);