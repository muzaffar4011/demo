"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[1260],{6148(e,n,o){o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4/voice-to-action","title":"Voice-to-Action","description":"Voice-to-action systems enable humanoid robots to understand spoken commands and translate them into robotic actions. This technology provides a natural interface for human-robot interaction.","source":"@site/docs/module-4/voice-to-action.md","sourceDirName":"module-4","slug":"/module-4/voice-to-action","permalink":"/physical-ai-humanoid-robotics/docs/module-4/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/physical-ai-humanoid-robotics/edit/main/docusaurus/docs/module-4/voice-to-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Voice-to-Action"},"sidebar":"tutorialSidebar","previous":{"title":"Module 4 - Vision-Language-Action (VLA)","permalink":"/physical-ai-humanoid-robotics/docs/module-4/"},"next":{"title":"LLM Cognitive Planning","permalink":"/physical-ai-humanoid-robotics/docs/module-4/llm-planning"}}');var t=o(4848),r=o(8453);const a={sidebar_position:2,title:"Voice-to-Action"},s="Voice-to-Action Systems for Humanoid Robots",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Voice-to-Action Systems",id:"introduction-to-voice-to-action-systems",level:2},{value:"Speech Recognition with OpenAI Whisper",id:"speech-recognition-with-openai-whisper",level:2},{value:"Basic Whisper Integration",id:"basic-whisper-integration",level:3},{value:"Advanced Speech Recognition Techniques",id:"advanced-speech-recognition-techniques",level:2},{value:"Keyword Spotting for Wake Word Detection",id:"keyword-spotting-for-wake-word-detection",level:3},{value:"Real-time Audio Processing",id:"real-time-audio-processing",level:3},{value:"Natural Language Understanding for Robotics",id:"natural-language-understanding-for-robotics",level:2},{value:"Command Parsing with Context",id:"command-parsing-with-context",level:3},{value:"Robust Voice Command Handling",id:"robust-voice-command-handling",level:2},{value:"Error Handling and Fallback Strategies",id:"error-handling-and-fallback-strategies",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Voice Command Interface",id:"voice-command-interface",level:3},{value:"Action Server for Voice Commands",id:"action-server-for-voice-commands",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Audio Processing Pipeline",id:"audio-processing-pipeline",level:3},{value:"Exercise",id:"exercise",level:2},{value:"Summary",id:"summary",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-to-action-systems-for-humanoid-robots",children:"Voice-to-Action Systems for Humanoid Robots"})}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action systems enable humanoid robots to understand spoken commands and translate them into robotic actions. This technology provides a natural interface for human-robot interaction."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this lesson, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement speech recognition systems for humanoid robots"}),"\n",(0,t.jsx)(n.li,{children:"Process natural language commands for robotic execution"}),"\n",(0,t.jsx)(n.li,{children:"Handle speech recognition errors and fallback mechanisms"}),"\n",(0,t.jsx)(n.li,{children:"Integrate voice commands with robotic action planning"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-voice-to-action-systems",children:"Introduction to Voice-to-Action Systems"}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action systems for humanoid robots involve:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Recognition"}),": Converting spoken language to text"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding"}),": Interpreting the meaning of commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Planning"}),": Converting commands into robotic actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution Monitoring"}),": Ensuring actions are performed correctly"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"For humanoid robots, voice-to-action systems must handle:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Noisy environments"}),"\n",(0,t.jsx)(n.li,{children:"Multiple speakers"}),"\n",(0,t.jsx)(n.li,{children:"Complex command structures"}),"\n",(0,t.jsx)(n.li,{children:"Real-time processing requirements"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"speech-recognition-with-openai-whisper",children:"Speech Recognition with OpenAI Whisper"}),"\n",(0,t.jsx)(n.h3,{id:"basic-whisper-integration",children:"Basic Whisper Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nimport openai\nimport numpy as np\nimport pyaudio\nimport wave\nimport threading\nimport queue\n\nclass VoiceToActionNode(Node):\n    def __init__(self):\n        super().__init__('voice_to_action_node')\n\n        # Publishers and subscribers\n        self.command_pub = self.create_publisher(String, '/robot_commands', 10)\n        self.status_pub = self.create_publisher(String, '/voice_status', 10)\n\n        # Audio recording parameters\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 44100\n        self.record_seconds = 5\n\n        # Initialize audio stream\n        self.audio = pyaudio.PyAudio()\n        self.audio_queue = queue.Queue()\n\n        # Start audio recording thread\n        self.recording = True\n        self.recording_thread = threading.Thread(target=self.record_audio)\n        self.recording_thread.start()\n\n        # Timer for processing audio\n        self.timer = self.create_timer(1.0, self.process_audio)\n\n    def record_audio(self):\n        \"\"\"Record audio from microphone\"\"\"\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        while self.recording:\n            data = stream.read(self.chunk)\n            self.audio_queue.put(data)\n\n        stream.stop_stream()\n        stream.close()\n\n    def process_audio(self):\n        \"\"\"Process recorded audio and convert to text\"\"\"\n        if not self.audio_queue.empty():\n            # Collect audio frames\n            frames = []\n            while not self.audio_queue.empty():\n                frames.append(self.audio_queue.get())\n\n            # Save to temporary WAV file for Whisper processing\n            wf = wave.open('temp_audio.wav', 'wb')\n            wf.setnchannels(self.channels)\n            wf.setsampwidth(self.audio.get_sample_size(self.format))\n            wf.setframerate(self.rate)\n            wf.writeframes(b''.join(frames))\n            wf.close()\n\n            # Process with Whisper\n            try:\n                with open('temp_audio.wav', 'rb') as audio_file:\n                    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n                    command_text = transcript.text\n\n                self.get_logger().info(f'Recognized: {command_text}')\n                self.process_command(command_text)\n            except Exception as e:\n                self.get_logger().error(f'Speech recognition error: {e}')\n                self.handle_recognition_error()\n\n    def process_command(self, command_text):\n        \"\"\"Process the recognized command text\"\"\"\n        # Publish the recognized command\n        cmd_msg = String()\n        cmd_msg.data = command_text\n        self.command_pub.publish(cmd_msg)\n\n        # Determine action based on command\n        action = self.parse_command(command_text)\n        if action:\n            self.execute_action(action)\n\n    def parse_command(self, command_text):\n        \"\"\"Parse natural language command into robotic action\"\"\"\n        command_text = command_text.lower().strip()\n\n        # Simple command parsing (in practice, use more sophisticated NLU)\n        if 'walk to' in command_text or 'go to' in command_text:\n            # Extract destination\n            destination = command_text.split('to')[-1].strip()\n            return {'action': 'navigate', 'target': destination}\n\n        elif 'pick up' in command_text or 'grasp' in command_text:\n            # Extract object\n            obj = command_text.split('up')[-1].strip() if 'up' in command_text else command_text.split('grasp')[-1].strip()\n            return {'action': 'grasp', 'object': obj}\n\n        elif 'wave' in command_text:\n            return {'action': 'gesture', 'type': 'wave'}\n\n        elif 'sit' in command_text:\n            return {'action': 'posture', 'type': 'sit'}\n\n        elif 'stand' in command_text:\n            return {'action': 'posture', 'type': 'stand'}\n\n        else:\n            return None\n\n    def execute_action(self, action):\n        \"\"\"Execute the parsed action on the humanoid robot\"\"\"\n        self.get_logger().info(f'Executing action: {action}')\n\n        # In a real implementation, this would send commands to the robot\n        # For now, just log the action\n        action_msg = String()\n        action_msg.data = f\"EXECUTE: {action}\"\n        self.status_pub.publish(action_msg)\n\n    def handle_recognition_error(self):\n        \"\"\"Handle speech recognition errors with fallback mechanisms\"\"\"\n        error_msg = String()\n        error_msg.data = \"SPEECH_RECOGNITION_ERROR\"\n        self.status_pub.publish(error_msg)\n\n        # In a real system, you might:\n        # - Ask for repetition: \"I didn't understand, could you repeat that?\"\n        # - Use alternative input methods\n        # - Implement error recovery strategies\n\n    def destroy_node(self):\n        \"\"\"Clean up resources\"\"\"\n        self.recording = False\n        if self.recording_thread.is_alive():\n            self.recording_thread.join()\n\n        self.audio.terminate()\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceToActionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"advanced-speech-recognition-techniques",children:"Advanced Speech Recognition Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"keyword-spotting-for-wake-word-detection",children:"Keyword Spotting for Wake Word Detection"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport numpy as np\nfrom scipy import signal\nimport threading\nimport queue\n\nclass KeywordSpotter:\n    def __init__(self, wake_word="robot"):\n        self.wake_word = wake_word\n        self.audio_buffer = queue.Queue()\n        self.detected = False\n\n    def detect_wake_word(self, audio_data):\n        """Simple keyword spotting algorithm"""\n        # In practice, use more sophisticated methods like:\n        # - Hidden Markov Models\n        # - Deep neural networks\n        # - Pre-trained keyword spotting models\n\n        # This is a simplified example\n        # Convert audio to text and check for wake word\n        # In real implementation, use audio features directly\n\n        return self.wake_word.lower() in audio_data.lower()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"real-time-audio-processing",children:"Real-time Audio Processing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport webrtcvad\nimport collections\n\nclass RealTimeAudioProcessor:\n    def __init__(self):\n        # Initialize WebRTC VAD (Voice Activity Detection)\n        self.vad = webrtcvad.Vad()\n        self.vad.set_mode(1)  # Aggressiveness mode: 0-3\n\n        # Audio parameters\n        self.sample_rate = 16000\n        self.frame_duration = 30  # ms\n        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)\n\n        # Ring buffer for audio\n        self.ring_buffer = collections.deque(maxlen=30)\n\n    def is_speech_detected(self, audio_frame):\n        """Detect if speech is present in audio frame"""\n        try:\n            return self.vad.is_speech(audio_frame, self.sample_rate)\n        except:\n            return False\n'})}),"\n",(0,t.jsx)(n.h2,{id:"natural-language-understanding-for-robotics",children:"Natural Language Understanding for Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"command-parsing-with-context",children:"Command Parsing with Context"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class NaturalLanguageProcessor:\n    def __init__(self):\n        self.robot_state = {\n            'location': 'start',\n            'carrying': None,\n            'battery': 100\n        }\n\n        # Define command patterns\n        self.command_patterns = {\n            'navigation': [\n                r'go to (.+)',\n                r'walk to (.+)',\n                r'move to (.+)',\n                r'navigate to (.+)'\n            ],\n            'manipulation': [\n                r'pick up (.+)',\n                r'grasp (.+)',\n                r'get (.+)',\n                r'take (.+)'\n            ],\n            'interaction': [\n                r'wave',\n                r'nod',\n                r'greet',\n                r'say hello'\n            ]\n        }\n\n    def parse_command_with_context(self, command_text):\n        \"\"\"Parse command considering current robot state\"\"\"\n        import re\n\n        # Update context based on command\n        for action_type, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, command_text, re.IGNORECASE)\n                if match:\n                    obj = match.group(1) if len(match.groups()) > 0 else None\n\n                    return {\n                        'action_type': action_type,\n                        'object': obj,\n                        'original_command': command_text,\n                        'context': self.robot_state.copy()\n                    }\n\n        return None\n"})}),"\n",(0,t.jsx)(n.h2,{id:"robust-voice-command-handling",children:"Robust Voice Command Handling"}),"\n",(0,t.jsx)(n.h3,{id:"error-handling-and-fallback-strategies",children:"Error Handling and Fallback Strategies"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class RobustVoiceHandler:\n    def __init__(self):\n        self.confidence_threshold = 0.7\n        self.retries = 3\n        self.context_history = []\n\n    def handle_command_with_confidence(self, command_text, confidence_score):\n        """Handle command based on confidence level"""\n        if confidence_score >= self.confidence_threshold:\n            return self.process_command(command_text)\n        else:\n            return self.request_clarification(command_text, confidence_score)\n\n    def request_clarification(self, command_text, confidence_score):\n        """Request user clarification for low-confidence commands"""\n        # Publish request for clarification\n        clarification_msg = f"Could you please repeat that? I understood: \'{command_text}\' with confidence {confidence_score:.2f}"\n\n        # In a real system, this would be spoken to the user\n        print(clarification_msg)\n\n        return {\'status\': \'clarification_requested\', \'message\': clarification_msg}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,t.jsx)(n.h3,{id:"voice-command-interface",children:"Voice Command Interface"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# ROS 2 interface definition for voice commands\n# voice_command_interface.yaml\nvoice_command_interface:\n  ros__parameters:\n    speech_recognition:\n      model: "whisper-large-v2"\n      language: "en"\n      sample_rate: 16000\n      chunk_size: 1024\n\n    nlu:\n      confidence_threshold: 0.7\n      max_command_length: 100\n      timeout_seconds: 5\n\n    fallback:\n      enable: true\n      max_retries: 3\n      alternative_input_topic: "/alternative_input"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"action-server-for-voice-commands",children:"Action Server for Voice Commands"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from rclpy.action import ActionServer\nfrom rclpy.action.server import CancelResponse\nimport rclpy\nfrom rclpy.node import Node\n\nclass VoiceCommandActionServer(Node):\n    def __init__(self):\n        super().__init__('voice_command_action_server')\n        self._action_server = ActionServer(\n            self,\n            VoiceCommand,  # Custom action message\n            'execute_voice_command',\n            self.execute_callback\n        )\n\n    def execute_callback(self, goal_handle):\n        \"\"\"Execute voice command with feedback\"\"\"\n        self.get_logger().info('Executing voice command...')\n\n        command = goal_handle.request.command\n\n        # Process command\n        result = self.process_voice_command(command)\n\n        if result.success:\n            goal_handle.succeed()\n        else:\n            goal_handle.abort()\n\n        return result\n"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"audio-processing-pipeline",children:"Audio Processing Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport concurrent.futures\n\nclass OptimizedAudioProcessor:\n    def __init__(self):\n        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n        self.audio_queue = asyncio.Queue()\n\n    async def process_audio_stream(self):\n        """Asynchronously process audio stream"""\n        while True:\n            audio_chunk = await self.audio_queue.get()\n\n            # Process in separate thread to avoid blocking\n            future = self.executor.submit(self.process_chunk, audio_chunk)\n\n            # Non-blocking result retrieval\n            result = await asyncio.get_event_loop().run_in_executor(None, future.result)\n\n            if result:\n                await self.handle_recognized_command(result)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"exercise",children:"Exercise"}),"\n",(0,t.jsx)(n.p,{children:"Create a voice-to-action system that:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Records audio from a microphone"}),"\n",(0,t.jsx)(n.li,{children:"Uses OpenAI Whisper for speech recognition"}),"\n",(0,t.jsx)(n.li,{children:"Parses commands for navigation and simple manipulation tasks"}),"\n",(0,t.jsx)(n.li,{children:"Implements error handling and fallback mechanisms"}),"\n",(0,t.jsx)(n.li,{children:"Integrates with ROS 2 for robot command execution"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action systems provide natural interfaces for humanoid robots, enabling communication through spoken language. Proper implementation requires robust speech recognition, natural language understanding, and error handling. The next lesson will cover LLM cognitive planning for more sophisticated command interpretation."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453(e,n,o){o.d(n,{R:()=>a,x:()=>s});var i=o(6540);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);